# Overview

The following content may constantly change. Please take note of the latest time that any changes happen to this folder.
If the date is somehow old and you cannot be sure if the following comments are up-to-date,
please feel free to raise any question in Risingwave's Slack community or create an issue on GitHub.

Thank you!

This folder has a number of files that include Risingwave's SQL for benchmarking by Nexmark. Suppose we have already
generated enough data by using `nexmark-bench` into Kafka.

`create_source.sql` creates a source in Risingwave. It defines the columns that will be read.

`create_source_with_watermark.sql` creates a source with watermark. 

`create_sinks.sql` include all of the queries that Risingwave currently supports in Nexmark except a few:
- Query 6 requires a SQL window function `AVG(Q.final) OVER
(PARTITION BY Q.seller ORDER BY Q.date_time ROWS BETWEEN 10 PRECEDING AND CURRENT ROW)`, which we are going to support
in the near future.
- Query 11 requires `Session Window`. This is on Risingwave's roadmap, but we will not support it soon as we haven't seen requests from any user.
- Query 12 requires `proctime()` function. Risingwave supports defining such column when creating the source but has not supported it as a function that can be used in the query.
- Query 13 also requires `proctime()` function.
- Query 19 requires outputting `rank` generated by `row_number()` window function. Risingwave supports `row_number()` but has not supported outputting the rank.

Additionally, Risingwave supported a few extra queries to cover more SQL operators:
- Query 101 to cover left outer join
- Query 102 to cover dynamic filter
- Query 103 to cover semi join
- Query 104 to cover anti join
- Query 105 to cover Top-N and group-by Top-N


`create_views.sql` dispatches different types of events to different views to make the development of queries easier.

`drop_sinks.sql`, `drop_views.sql` and `drop_source.sql` cleans up the environment.

### Steps

After all the data have been generated into Kafka, we:
- `create_source.sql` or `create_source_with_watermark.sql`
- `create_views.sql`
- pick the query you want to run from `create_sinks.sql`
- `drop_views.sql`, then `drop_sinks.sql`, and finally `drop_source.sql` to clean up

### Remarks
Please make sure that the parallelism of the streaming job is equal to the number of partitions of the Kafka topic that Risingwave is reading from.
